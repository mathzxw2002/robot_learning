
# VLA




# Agent

https://dobby.now \

./agent_web_camera.mp4

# LLMs

https://github.com/QwenLM/Qwen2.5-VL \

https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html \

<img width="3000" height="2093" alt="image" src="https://github.com/user-attachments/assets/e56a81d3-f818-4afa-ad23-96ebbc22a6a3" />
So, in this article, rather than writing about benchmark performance or training algorithms, I will focus on the architectural developments that define todayâ€™s flagship open models.


ThinkDiff: https://mizhenxing.github.io/ThinkDiff/ \
https://github.com/MiZhenxing/ThinkDiff \
<img width="3802" height="918" alt="image" src="https://github.com/user-attachments/assets/8b088401-c3c7-4fb7-ba3d-a7ecb1257228" />


SmolLM3-3B: https://github.com/huggingface/smollm \
Everything about the SmolLM and SmolVLM family of models \
https://huggingface.co/HuggingFaceTB \


Training extremely large neural networks across thousands of GPUs. \
https://www.jeremyjordan.me/distributed-training/


Anthropic's educational courses:\
https://github.com//anthropics/courses

Reinforcement Learning from Human Feedback : \
https://rlhfbook.com \



Kimi

Kimi K2æŠ€æœ¯æŠ¥å‘Šï¼šæœ€æ–°å¼€æºæ™ºèƒ½ä½“å¤§æ¨¡å‹ï¼Œåˆ·æ–°éæ€è€ƒæ¨¡å¼æ€§èƒ½æ–°é«˜åº¦ã€‚
â€¢ 1.04ä¸‡äº¿å‚æ•°MoEæ¶æ„ï¼Œæ¿€æ´»å‚æ•°è¾¾320äº¿ï¼ŒåŸºäºMuonClipä¼˜åŒ–å™¨å®ç°15.5ä¸‡äº¿é«˜è´¨é‡tokené¢„è®­ç»ƒæ— æŸå¤±å³°å€¼ã€‚
â€¢ åˆ›æ–°QK-Clipæœºåˆ¶ï¼Œç¨³å®šæ§åˆ¶æ³¨æ„åŠ›logitsçˆ†ç‚¸ï¼Œç¡®ä¿å¤§è§„æ¨¡è®­ç»ƒç¨³å®šé«˜æ•ˆã€‚
â€¢ å¤§è§„æ¨¡åˆæˆå·¥å…·è°ƒç”¨æ•°æ®ä¸å¤šé˜¶æ®µå¼ºåŒ–å­¦ä¹ ç›¸ç»“åˆï¼Œå¼ºåŒ–æ¨¡å‹è‡ªä¸»æ„ŸçŸ¥ã€è§„åˆ’ã€æ¨ç†ä¸è¡ŒåŠ¨èƒ½åŠ›ã€‚
â€¢ å¤šé¡¹æƒå¨åŸºå‡†æµ‹è¯•é¢†å…ˆå¼€æºåŠé—­æºå¯¹æ‰‹ï¼šTau2-Bench 66.1ã€ACEBench 76.5ã€SWE-Bench Verified 65.8ï¼Œå±•ç°å“è¶Šç¼–ç ã€æ•°å­¦ä¸æ¨ç†å®åŠ›ã€‚
â€¢ çµæ´»é«˜æ•ˆçš„è®­ç»ƒä¸æ¨ç†æ¶æ„æ”¯æŒ128kè¶…é•¿ä¸Šä¸‹æ–‡ï¼Œå…¼é¡¾æ€§èƒ½ä¸æˆæœ¬ï¼Œæ¨åŠ¨æ™ºèƒ½ä½“æŠ€æœ¯å‰æ²¿ã€‚
â€¢ å®Œå–„å®‰å…¨ç­–ç•¥ä¸çº¢é˜Ÿè¯„ä¼°ï¼Œä¿éšœç”Ÿæˆå†…å®¹å¯é ä¸åˆè§„ã€‚
â€¢ å¼€æºåŸºç¡€åŠåè®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹ï¼ŒåŠ©åŠ›ç¤¾åŒºå…±å»ºæ™ºèƒ½ä½“æœªæ¥ã€‚
æŠ€æœ¯æŠ¥å‘ŠğŸ‘‰ github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf
æ¨¡å‹ä¸‹è½½ğŸ‘‰ huggingface.co/moonshotai/Kimi-K2-Instruct




# MCP


# Industrial Vision


PyVision: Agentic Vision with Dynamic Tooling \
paper: https://arxiv.org/pdf/2507.07998  \
homepage: https://agent-x.space/pyvision/ \
code: https://github.com/agents-x-project/PyVision \
online demo: https://huggingface.co/spaces/Agents-X/PyVision


# Course

https://ernestryu.com/courses/RL-LLM.html \


CS25: Transformers United V5: https://web.stanford.edu/class/cs25 \

Stanford CS25: V2 I Introduction to Transformers w/ Andrej Karpathy: https://www.youtube.com/watch?v=XfpMkf4rD6E \



# Computer Vision

About
[NeurIPS 2024] Depth Anything V2. A More Capable Foundation Model for Monocular Depth Estimation \
https://github.com/DepthAnything/Depth-Anything-V2



Describe Anything: Detailed Localized Image and Video Captioning \
code: https://github.com/NVlabs/describe-anything \
homepage: https://describe-anything.github.io \


Master AI Agentic Engineering - build autonomous AI Agents: https://github.com/ed-donner/agents \
6 week journey to code and deploy AI Agents with OpenAI Agents SDK, CrewAI, LangGraph, AutoGen and MCP \

<img width="1536" height="1024" alt="image" src="https://github.com/user-attachments/assets/f203ec73-257e-4dd9-b538-5f33c541f175" />



Ovis-U1: Unified Understanding, Generation, and Editing: An unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. \
https://github.com/AIDC-AI/Ovis-U1 \
demo : https://huggingface.co/spaces/AIDC-AI/Ovis-U1-3B \


CS336: Language Modeling from Scratch
Stanford / Spring 2025: https://stanford-cs336.github.io/spring2025/

Hunyuan3D-2.1: https://github.com/Tencent-Hunyuan/Hunyuan3D-2.1 \
3d.hunyuan.tencent.com/ \



